{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "%matplotlib inline\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data and convert it to pandas\n",
    "data=[]\n",
    "with open('../data/class-data-with-all-tests-and-concept-check-NO_IDENTIFIERS.csv', mode='r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    data = list(csv_reader)\n",
    "    \n",
    "headers = data[0]\n",
    "rest = data[1:]\n",
    "\n",
    "everything = pd.DataFrame(rest, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_int(dat):\n",
    "    return dat.apply(lambda x: int(x) if x else 0)\n",
    "def score1a(data):\n",
    "    return (data['1a'] == 'c').astype(int)\n",
    "def score1b(data):\n",
    "    a,b,c,d,e,f = as_int(data['1b.a']),as_int(data['1b.b']),as_int(data['1b.c']),as_int(data['1b.d']),as_int(data['1b.e']),as_int(data['1b.f'])\n",
    "    return (a+(1-b)+c+(1-d)+e+(1-f))/6\n",
    "def score2a(data):\n",
    "    return (data['2a'] == 'a').astype(int)\n",
    "def score2b(data):\n",
    "    return (data['2b'] == '0').astype(int)\n",
    "def score2c(data):\n",
    "    a,b,c,d,e = map(as_int, (data['2c.a'],data['2c.b'],data['2c.c'],data['2c.d'],data['2c.e']))\n",
    "    return ((1-a)+b+(1-c)+(1-d)+(1-e))/5\n",
    "def score3(data):\n",
    "    a,b,c,d,e,f = map(as_int, (data['3.a'],data['3.b'],data['3.c'],data['3.d'],data['3.e'],data['3.f']))\n",
    "    return ((1-a)+b+c+d+(1-e)+f)/6\n",
    "def score4a(data):\n",
    "    return (data['4a'] == 'd').astype(int)\n",
    "def score5(data):\n",
    "    a,b,c,d,e = map(as_int, (data['5.a'],data['5.b'],data['5.c'],data['5.d'],data['5.e']))\n",
    "    return (a+(1-b)+(1-c)+d+(1-e))/5\n",
    "def score6a(data):\n",
    "    return (data['6a'] == '1').astype(int)\n",
    "def score6b(data):\n",
    "    return (data['6b'] == 'c').astype(int)\n",
    "def score7(data):\n",
    "    return (data['7'] == 'd').astype(int)\n",
    "def score8(data):\n",
    "    a,b,c,d,e,f = map(as_int, (data['8.a'],data['8.b'],data['8.c'],data['8.d'],data['8.e'],data['8.f']))\n",
    "    return ((1-a)+(1-b)+(1-c)+(1-d)+e+(1-f))/6\n",
    "def score9a(data):\n",
    "    a,b,c,d,e,f,g,h,i = map(as_int, (data['9a.a'],data['9a.b'],data['9a.c'],data['9a.d'],data['9a.e'],data['9a.f'],data['9a.g'],data['9a.h'],data['9a.i']))\n",
    "    return (a+(1-b)+c+d+(1-e)+f+(1-g)+(1-h)+(1-i))/9\n",
    "def score9b(data):\n",
    "    a,b,c,d,e,f = map(as_int, (data['9b.a'],data['9b.b'],data['9b.c'],data['9b.d'],data['9b.e'],data['9b.f']))\n",
    "    return ((1-a)+(1-b)+(1-c)+(1-d)+e+(1-f))/6\n",
    "def score10(data):\n",
    "    a,b,c,d,e = map(as_int, (data['10.a'],data['10.b'],data['10.c'],data['10.d'],data['10.e']))\n",
    "    return ((1-a)+(1-b)+c+d+e)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment = pd.DataFrame({\"1a\":score1a(everything),\"1b\":score1b(everything),\"2a\":score2a(everything),\n",
    "                           \"2b\":score2b(everything),\"2c\":score2c(everything),\"3\":score3(everything),\n",
    "                           \"4a\":score4a(everything),\"5\":score5(everything),\"6a\":score6a(everything),\n",
    "                           \"6b\":score6b(everything),\"7\":score7(everything),\"8\":score8(everything),\n",
    "                          \"9a\":score9a(everything),\"9b\":score9b(everything),\"10\":score10(everything)})\n",
    "\n",
    "assessment.describe().loc[['mean','std']].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by formatting our data before feeding it into a densely connected 3 layer network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 264\n",
    "\n",
    "# Convert dataframe back to numpy array\n",
    "assessement_np = assessment.to_numpy()\n",
    "\n",
    "# Separate training data from testing data and grab labels\n",
    "train_data = assessement_np[:train_size]\n",
    "train_labels = everything['Final.total'].to_numpy()[:train_size].astype(np.float)\n",
    "\n",
    "test_data = train_data\n",
    "test_labels = train_labels\n",
    "\n",
    "#test_data = assessement_np[train_size:]\n",
    "#test_labels = everything['Final.total'].to_numpy()[train_size:].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Dense(1, activation='relu', input_shape=(15,))\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mean_absolute_error',\n",
    "  metrics=['mean_absolute_error'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_data,\n",
    "  train_labels,\n",
    "  epochs=1000,\n",
    "  batch_size=32,\n",
    ")\n",
    "\n",
    "#Evaluate the model.\n",
    "model.evaluate(\n",
    "    test_data,\n",
    "    test_labels\n",
    ")\n",
    "\n",
    "# Save the model to disk.\n",
    "#model.save_weights('model.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('model.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "#predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "#print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "#print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our predications against the true values\n",
    "test_data = assessement_np[21:25]\n",
    "model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(everything['Final.total'].to_numpy()[21:25].astype(np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab individual weights of the network\n",
    "model.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-Do \n",
    "1. Investigate and break the model in the ways we know it should\n",
    "2. Implement new nets for the individual select all the apply questions\n",
    "3. Check that midterm 1 and 2 are both roughly equivalent indicators of final exam scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CCNN_virtualenv",
   "language": "python",
   "name": "ccnn_virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
